{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. segment\n",
    "## 2.1 NLTK segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import MWETokenizer\n",
    "from nltk.corpus import wordnet\n",
    "wnl = WordNetLemmatizer()\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_path = './result'\n",
    "data = pd.read_excel(\"./seg.xlsx\").astype(str)  # content type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cut(mytext):\n",
    "    #de-punctuate\n",
    "    for c in string.punctuation:\n",
    "        if c != '-':\n",
    "            mytext = mytext.replace(c, ' ')\n",
    "    #Word segmentation, add custom phrases, remove stop words\n",
    "    tokenizer = MWETokenizer(\n",
    "        [('Python', 'programs'), ('a', 'little', 'bit'), ('a', 'lot')], separator='-')\n",
    "    wordlist = tokenizer.tokenize(nltk.word_tokenize(mytext))\n",
    "    filtered = [w for w in wordlist if w not in stopwords.words('english')]\n",
    "    #Part of speech\n",
    "    refiltered = nltk.pos_tag(filtered)\n",
    "    #Morphology reduction\n",
    "    lemmas_sent = []\n",
    "    for wordtag in refiltered:\n",
    "        wordnet_pos = get_word_pos(wordtag[1]) or wordnet.NOUN\n",
    "        word = wnl.lemmatize(wordtag[0], pos=wordnet_pos)\n",
    "        # print(word)\n",
    "        lemmas_sent.append(word)  # Morphology reduction\n",
    "    # print(lemmas_sent)\n",
    "    return (\" \").join(lemmas_sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"content_cutted\"] = data.post.apply(word_cut)\n",
    "\n",
    "print(data[\"content_cutted\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save all word segmentation results to save subsequent time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel(\"./seg.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    tword = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        topic_w = \" \".join([feature_names[i]\n",
    "                            for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        tword.append(topic_w)\n",
    "        print(topic_w)\n",
    "    return tword\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 1000  # Extract 1000 feature words\n",
    "tf_vectorizer = CountVectorizer(strip_accents='unicode',\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english',\n",
    "                                max_df=0.5,\n",
    "                                min_df=10)\n",
    "\n",
    "# tf = tf_vectorizer.fit_transform(data.content_cutted)\n",
    "data['age'] = data['age'].astype(int)\n",
    "tf = tf_vectorizer.fit_transform(data.loc[data['age'] <= 20, 'content_cutted'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When building a topic model, in addition to the dictionary and corpus, we also need to specify parameters for the model, such as the number of topics.\n",
    "- num_topics represents the number of topics to generate.\n",
    "- chunksize is the number of documents used in each training chunk.\n",
    "- update_every determines the frequency of updating the model parameters.\n",
    "- passes is the total number of training passes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 6 #topic number\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=50,\n",
    "                                learning_method='batch',\n",
    "                                learning_offset=50,\n",
    "                                 doc_topic_prior=0.1,\n",
    "                                 topic_word_prior=0.01,\n",
    "                               random_state=0)\n",
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above LDA model is constructed by four different topics, in which each topic is a combination of keywords, and each keyword contributes a certain weight to the topic, and the weight reflects the contribution degree of the keyword to the subject.\n",
    "\n",
    "num_word represents the number of key words for each topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Output words for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 20\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "topic_word = print_top_words(lda, tf_feature_names, n_top_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Output each article corresponding to the topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda.transform(tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = []\n",
    "for t in topics:\n",
    "    topic.append(\"Topic #\"+str(list(t).index(np.max(t))))\n",
    "data['The topic number with the highest probability'] = topic\n",
    "data['Each topic corresponds to a probability'] = list(topics)\n",
    "data.to_excel(\"./result/data_topic_under6.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "# The pandas package version must be greater than 1.3.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "pic = pyLDAvis.sklearn.prepare(lda, tf, tf_vectorizer)\n",
    "pyLDAvis.display(pic)\n",
    "pyLDAvis.save_html(pic, 'lda_pass'+str(n_topics)+'_below.html')\n",
    "\n",
    "#Go to the working path and look for the saved html file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 Perplex calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plexs = []\n",
    "scores = []\n",
    "n_max_topics = 10\n",
    "for i in range(1, n_max_topics):\n",
    "    print(i)\n",
    "    lda = LatentDirichletAllocation(n_components=i, max_iter=50,\n",
    "                                    learning_method='batch',\n",
    "                                    learning_offset=50, random_state=0)\n",
    "    lda.fit(tf)\n",
    "    plexs.append(lda.perplexity(tf))\n",
    "    scores.append(lda.score(tf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_t = 9  # The value on the far right of the interval. Note: cannot be greater than n_max_topics\n",
    "x = list(range(1, n_t+1))\n",
    "plt.plot(x, plexs[0:n_t])\n",
    "plt.xlabel(\"number of topics\")\n",
    "plt.ylabel(\"perplexity\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x, plexs[0:n_t])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "#from .data_utils import *\n",
    "import jieba\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import string\n",
    "os.chdir(\"./\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    1. Load corpus and divide words\n",
    "'''\n",
    "data = pd.read_excel(\"./dataForCode/output.xlsx\")\n",
    "corpus = []\n",
    "\n",
    "# bigram\n",
    "def segment_bigram(text): return \" \".join(\n",
    "    [word + text[idx + 1] for idx, word in enumerate(text) if idx < len(text) - 1])\n",
    "\n",
    "\n",
    "for line in data['post'].astype(str):\n",
    "# for line in data.loc[data['occupation'] == 'Student', 'post'].astype(str):\n",
    "    # de-punctuate\n",
    "    corpus.append(word_cut(line.strip()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    1. Load corpus and divide words\n",
    "'''\n",
    "data = pd.read_excel(\"./seg.xlsx\")\n",
    "corpus = []\n",
    "\n",
    "# for line in data.content_cutted.astype(str):\n",
    "data['age'] = data['age'].astype(int)\n",
    "\n",
    "for line in data.loc[data['age'] <= 20, 'content_cutted'].astype(str):\n",
    "    # 去掉标点符号\n",
    "    corpus.append(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    2、Calculate tf-idf set as weight\n",
    "'''\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus[:10000]))\n",
    "\n",
    "''' \n",
    "3. Obtain all word features in the word bag model\n",
    "If the number of features is very large, the dimension can be reduced according to the weight\n",
    "'''\n",
    "\n",
    "word = vectorizer.get_feature_names()\n",
    "print(\"word feature length: {}\".format(len(word)))\n",
    "\n",
    "''' \n",
    "    4、The process of vectorizing text is achieved by exporting the weights here, and each row in the matrix is a vector representation of a document\n",
    "'''\n",
    "tfidf_weight = tfidf.toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Draw Inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "inertia = []\n",
    "scores = []\n",
    "n_max_topics = 10\n",
    "for i in range(1, n_max_topics):\n",
    "    print(i)\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "    kmeans.fit(tfidf_weight)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "n_t = 9  # The value on the far right of the interval. Note: cannot be greater than n_max_topics\n",
    "x = list(range(1, n_t+1))\n",
    "plt.plot(x, inertia[0:n_t])\n",
    "plt.xlabel(\"number of topics\")\n",
    "plt.ylabel(\"inertia\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x, inertia[0:n_t])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    5、Clustering of vectors\n",
    "'''\n",
    "\n",
    "#   Specifies that it is divided into n classes\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(tfidf_weight)\n",
    "\n",
    "# Print out the center points of each cluster\n",
    "print(\"Center point coordinates:\")\n",
    "print(kmeans.cluster_centers_)\n",
    "# for index, label in enumerate(kmeans.labels_, 1):\n",
    "#     print(\"index: {}, label: {}\".format(index, label))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sum of the square distance of the sample from its nearest cluster center is used to judge the accuracy of the classification. The smaller the value, the better\n",
    "# The hyperparameter n_clusters of k-means can be evaluated by this value\n",
    "print(\"Effect evaluation value:\")\n",
    "print(\"inertia: {}\".format(kmeans.inertia_))\n",
    "\n",
    "##Save the result to excel\n",
    "data['label'] = kmeans.labels_\n",
    "data.to_excel(\"./result/data_labeled_student.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    6、visualization\n",
    "'''\n",
    "\n",
    "# T-SNE algorithm is used to reduce the dimension of weights, which is more accurate than PCA algorithm, but takes a long time\n",
    "tsne = TSNE(n_components=2)\n",
    "decomposition_data = tsne.fit_transform(tfidf_weight)\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for i in decomposition_data:\n",
    "    x.append(i[0])\n",
    "    y.append(i[1])\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = plt.axes()\n",
    "plt.scatter(x, y, c=kmeans.labels_, marker=\"x\")\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()\n",
    "plt.savefig('./result/sample_below3.png', aspect=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "6f5583cf1d9466b5c27e75c89cc6b383bed5736d6b16c51c8074d8690011a952"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
